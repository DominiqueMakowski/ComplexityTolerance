---
title             : "**Optimal Selection of Tolerance *r* for Entropy Indices**"
shorttitle        : "Optimal Tolerance"
author: 
  - name          : "Dominique Makowski"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    address       : "HSS 04-18, 48 Nanyang Avenue, Singapore"
    email         : "dmakowski@ntu.edu.sg"
    orcid         : 0000-0001-5375-9967
      
affiliation:
  - id            : "1"
    institution   : "School of Social Sciences, Nanyang Technological University, Singapore"
authornote: |
  Correspondence concerning this article should be addressed to Dominique Makowski, HSS 04-18, 48 Nanyang Avenue, Singapore (dom.makowski@gmail.com).
abstract: |
  The tolerance threshold *r* is a key parameter of several entropy algorithms (e.g., *SampEn*). Unfortunately, the gold standard method to estimate its optimal value - the one that maximizes *ApEn* - is computationally costly, prompting users to rely to cargo-cult rules-of-thumb such as 0.2 * SD. In this study, we first investigated the possibility of using related quantities, namely the amount of Nearest Neighbours (*NN*) and the Recurrence Rate (*RR*), to approximate the optimal *r* value. Secondly, we established a new heuristic, based only on the signal's SD and the embedding dimension *m* (optimal *r* = -0.032 + 0.1497 \* *m*), which was superior to other existing heuristics. All the methods of optimal tolerance *r* estimation used in this study are available in the *NeuroKit2* Python software (Makowski et al., 2021).
  
keywords          : "chaos, complexity, fractal, physiology, tolerance"
wordcount         : "1156"
bibliography      : "bibliography.bib"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : yes
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
csl: utils/mdpi.csl
header-includes:
  - \usepackage[labelfont=bf, font={scriptsize, color=gray}]{caption}
editor_options: 
  chunk_output_type: console
---



```{r, echo = FALSE, warning=FALSE, message=FALSE}
# options and parameters
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  dpi = 450,
  fig.width = see::golden_ratio(9),
  fig.height = 9,
  fig.path = "figures/"
)

cache <- TRUE
```


## Introduction

Complexity analysis is a growing approach to physiological signals, including cardiac [e.g., Heart Rate Variability, @pham2021heart] and brain activity [@lau2021brain]. It is an umbrella term for the usage of various complexity indices that quantify concepts such as chaos, entropy, fractal dimension, randomness, predictability, and information. Importantly, some of the most popular indices of entropy (e.g., *ApEn*, *SampEn*, their fuzzy and multiscale variations) and recurrence quantification analysis (RQA), rely on the same subset of parameters. Namely, these are the delay $\tau$, the embedding dimension *m*, and the tolerance *r*, which are critical to accurately capture the space in which complexity becomes quantifiable. Unfortunately, despite the existence of methods to estimate optimal values for these parameters depending on the signal at hand, their choice often relies on simple heuristics and cargo-cult conventions.

Such is the case of the tolerance threshold *r*, which typically corresponds to the minimal distance required to assigning two points in a state-space as belonging to the same state. It directly impacts the amount of "recurrences" of a system and its tendency to revisit past states, which is the base metric for the calculation of the aforementioned entropy indices. Despite its importance, it is often selected as a function of the standard deviation (SD) of the signal, with (in)famous magic values including $0.1$ or $0.2*SD$ [@pincus1992approximate]. One of the reason for the longevity of such approach is 1) the past literature (as many past studies used it, it becomes easier to justify the choice of the same values) and 2) the fact that other approaches to estimate the optimal *r* are computationally costly.

<!-- Figure of 2D attractor with radius  -->

The aim of the present study is to investigate the relationship between different methods for optimal tolerance *r* estimation. The ground-truth method used is the tolerance value corresponding to a maximal value of Approximate Entropy - *ApEn* [@chen2008parameter; @lu2008automatic; @chon2009approximate]. As this method is computationally expensive, the objective is to see whether any heuristic proxies can be used to satisfyingly approximate $r_{maxApEn}$.


## Methods

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(easystats)
library(patchwork)



df <- data.frame()
for(i in 1:10) {
  df <- rbind(df, read.csv(paste0("data/data_Tolerance_part", i, ".csv")))
}

df <- df |> 
  mutate(Iter = paste0(Signal, Noise_Intensity, Noise_Type, Length)) |> 
  group_by(Method, Dimension, Iter) |> 
  mutate(Score_N = normalize(Score)) |> 
  ungroup()
```


For *n* = `r length(unique(df$Iter))` combinations of different signal types and lengths, as well as noise types and intensities (the procedure used was the same as in ..., and the data generation code is available at **https://github.com/DominiqueMakowski/ComplexityTolerance**), we computed 3 different scores as a function of difference tolerance values (expressed in SDs of the signal): Approximate Entropy (*ApEn*), which peak is used to estimate the optimal tolerance level; the average number of Nearest Neighbours *NN*, which is the underlying quantity used by several entropy algorithms; and the Recurrence Rate *RR*, one of the core index of recurrence quantification analysis (RQA). These 3 scores were computed for time-delay embedding spaces ranging from of 1 to 9 embedding dimensions *m*.


The aim of the analysis is to 1) investigate the possibility of using alternative scores, namely *RR* and *NN*, to approximate the location of the *ApEn* peak; 2) establish a new heuristic based on signal's SD and embedding 
dimension *m*; and 3) compare all of these approximations with other existing heuristics such as *0.2 SD*, *Chon* [@chon2009approximate], and the *Schötzel* method ($1.3334 + 0.5627 * log(dimension)$) implemented in the package *nolds* [@scholzel2019nolds].

<!-- (The rationale is that the chebyshev distance (used in various metrics) rises logarithmically with increasing dimension. ``0.5627 * np.log(dimension) + 1.3334`` is the logarithmic trend line for the chebyshev distance of vectors sampled from a univariate normal distribution. A constant of `0.1164`` is used so that ``tolerance = 0.2 * SDs`` for ``dimension = 2``) -->

## Results


### Descriptive Results


```{r warning=FALSE, message=FALSE, cache=cache}
p <- df |> 
  mutate(group = paste0(Dimension, Method, Iter),
         m = Dimension) |> 
  filter(Method %in% c("Approximate Entropy", "Recurrence Rate", "Nearest Neighbours")) |> 
  ggplot(aes(x = Tolerance, y = Score_N, color = Method)) +
  geom_line(aes(group=group), alpha=0.05, size=0.20) +
  facet_wrap(~m, labeller=purrr::partial(label_both, sep = " = ")) +
  scale_x_continuous(expand=c(0, 0)) +
  scale_y_continuous(expand=c(0, 0), labels = scales::percent) +
  scale_color_manual(values=c("Approximate Entropy" = "#9C27B0", "Nearest Neighbours" = "#2196F3", "Recurrence Rate" = "#FF9800")) +
  theme_modern() + 
  theme(strip.text = element_text(face="italic"),
        axis.title.y = element_blank()) +
  labs(x = expression("Tolerance "~italic("r")~" (in signal's SD)"), color = "") +
  guides(colour = guide_legend(override.aes = list(alpha = 1, size=1)))
# p
ggsave("figures/fig1-1.png", width=see::golden_ratio(9), height=9, dpi=300)
```
```{r fig1, echo=FALSE, fig.cap="Caption.", message=FALSE, warning=FALSE, cache=FALSE, out.width="100%", eval=FALSE}
knitr::include_graphics("figures/fig1-1.png")
```

**Figure 1** shows the normalized value of Approximate Entropy *ApEn*, the amount of nearest neighbours *NN* and the Recurrence Rate *RR* as a function of tolerance *r*. As expected, the value of *ApEn* peaks at certain values of *r* (hence its usage as an indicator of the optimal tolerance). The location of this peak seems strongly impacted by the embedding dimension *m*, getting more variable as *m* increases. Does this peak consistently correspond to certain values of *NN* and *RR*?



### Using NN and RR

```{r message=FALSE, warning=FALSE, include=FALSE}
data <- df |> 
  group_by(Dimension, Length, Iter) |> 
  filter(Method %in% c("Recurrence Rate", "Nearest Neighbours")) |> 
  filter(Tolerance == unique(Optimal_maxApEn)) |> 
  ungroup()

m_NN0 <- lm(Tolerance ~ Score, data=filter(data, Method=="Nearest Neighbours"))
m_NN1 <- lm(Tolerance ~ Score + Dimension, data=filter(data, Method=="Nearest Neighbours"))
m_NN2 <- lm(Tolerance ~ Score + log(Dimension), data=filter(data, Method=="Nearest Neighbours"))
m_NN3 <- lm(Tolerance ~ Score * Dimension, data=filter(data, Method=="Nearest Neighbours"))
m_NN4 <- lm(Tolerance ~ Score * log(Dimension), data=filter(data, Method=="Nearest Neighbours"))

test_performance(m_NN0, m_NN1, m_NN2, m_NN3, m_NN4, reference=2)

m_NN5 <- lm(Tolerance ~ Score + Length + Dimension, data=filter(data, Method=="Nearest Neighbours"))
m_NN6 <- lm(Tolerance ~ Score * Length + Dimension, data=filter(data, Method=="Nearest Neighbours"))

test_bf(m_NN5, m_NN1, m_NN6)

m_NN <- m_NN2
```

```{r warning=FALSE, message=FALSE, include=FALSE}
m_RR0 <- lm(Score ~ 1, data=filter(data, Method=="Recurrence Rate"))
RR0 <- coef(m_RR0)

m_RR1 <- lm(Score ~ Dimension, data=filter(data, Method=="Recurrence Rate"))
m_RR2 <- lm(Score ~ log(Dimension), data=filter(data, Method=="Recurrence Rate"))
test_performance(m_RR0, m_RR1, m_RR2, reference=2)

m_RR <- m_RR2
```


```{r fig1b, warning=FALSE, message=FALSE, eval=FALSE}
means <- estimate_means(m_RR2, at = list("Dimension" = unique(data$Dimension))) |> 
  mutate(Method = "Recurrence Rate") |> 
  rbind(
    estimate_means(m_NN1, at = list("Dimension" = unique(data$Dimension))) |> 
      mutate(Method = "Nearest Neighbours"))  |> 
  mutate(m = paste0("m = ", Dimension),
         m = fct_rev(as.factor(m)))

data |> 
  mutate(m = paste0("m = ", Dimension),
         m = fct_rev(as.factor(m))) |>
  ggplot(aes(y = Optimal_maxApEn, x = Score)) +
  geom_point2(aes(color=m), alpha=0.33) +
  geom_vline(data=means, aes(xintercept=Mean, group=Dimension)) +
  facet_grid(m ~ Method, switch="x") +  # labeller=purrr::partial(label_both, sep = " = "), 
  scale_color_viridis_d(option="inferno", direction=-1) +
  guides(color="none") +
  theme_modern() +
  theme(strip.placement = "outside",
        axis.title.x = element_blank())
```

In order to assess whether the amount of nearest neighbours *NN* and the Recurrence Rate *RR* can be used to approximate the optimal tolerance threshold *r* (as estimated by *max. ApEn*), we fitted for each index 3 regression models to predict the index' value that corresponds to the location of *max. ApEn*: one without the embedding dimension *m* as predictor, one with it, and one with the dimension's logarithm. For both *NN* and *RR*, the model with the log-transform dimension as predictor was the best $BF_{10} > 1000$. However, *NN* did not share a strong relationship with the embedding dimension, as the explained variance of its (best-performing) model was low ($R^2$ = `r insight::format_value(performance::r2(m_NN)$R2, as_percent=TRUE)`). It was higher for the model based on *RR* ($R^2$ = `r insight::format_value(performance::r2(m_RR)$R2, as_percent=TRUE)`). The models were as follows: 

`r equatiomatic::extract_eq(m_RR, swap_var_names=c("Score" = "NN"), use_coefs = TRUE, ital_vars=TRUE, coef_digits=4)`

`r equatiomatic::extract_eq(m_RR, swap_var_names=c("Score" = "RR"), use_coefs = TRUE, ital_vars=TRUE, coef_digits=4)`

According to these models, for an embedding dimension of 2, the target *NN* and *RR* values are `r insight::format_value(predict(m_NN, data.frame(Dimension=2)), digits=1, as_percent=TRUE)` and `r insight::format_value(predict(m_RR, data.frame(Dimension=2)), digits=1, as_percent=TRUE)`, respectively.


### New Heuristic

Because computing *RR* or *NN* is also an expensive procedure, we also attempted at validating a new heuristic based only on the signal's *SD* and the embedding dimension *m*.

```{r warning=FALSE, message=FALSE, include=FALSE}
data <- df |> 
  group_by(Dimension, Length, Iter) |> 
  summarise(maxApEn = mean(Optimal_maxApEn, na.rm=TRUE)) 

m1 <- lm(maxApEn ~ Dimension, data=data)
m2 <- lm(maxApEn ~ log(Dimension), data=data)
test_performance(m1, m2)

m <- m1
```


```{r fig2, warning=FALSE, message=FALSE}
library(ggdist)

pred <- estimate_relation(m, at = list("Dimension"=unique(data$Dimension)))

formula <- equatiomatic::extract_eq(m, 
                                    raw_tex=TRUE, 
                                    swap_var_names=c("maxApEn" = "r"),
                                    use_coefs = TRUE, 
                                    ital_vars=TRUE, 
                                    coef_digits=3)
  
data |> 
  mutate(Dimension = as.factor(Dimension),
         Length = as.factor(Length)) |> 
  ggplot(aes(x=maxApEn, y=Dimension)) +
  ggdist::stat_halfeye(aes(fill=Dimension), adjust =4, point_interval="mode_qi", color = "#607D8B", normalize="groups") +
  # geom_density_ridges(aes(fill=Dimension), color = NA) +
  # stat_density_ridges(aes(fill=Dimension), quantile_lines = TRUE, quantiles = 2, color = NA)
  geom_line(data=pred, aes(x=Predicted), color="red", size=1, show.legend=FALSE) +
  coord_flip() +
  annotate(geom="text", x=1.5, y=1.2, label=latex2exp::TeX(formula, italic=TRUE), hjust=0, color="black") +
  scale_fill_viridis_d(option="inferno") +
  scale_x_continuous(expand=c(0, 0)) +
  scale_y_discrete(expand=c(0, 0)) +
  theme_modern() +
  guides(fill="none", group="none") +
  labs(x = "Optimal Tolerance (based on max. ApEn)", y="Embedding Dimension")
```

Selecting the tolerance based on the signal's SD alone makes hardly sense, as the embedding dimension has a strong impact on it. We fitted two models to predict the optimal tolerance (as estimated by *max. ApEn*), with the embedding dimension and its log-transformation as predictors, respectively. The model without the log-transformation performed significantly better ($BF_{10} > 1000$), with an explained variance of `r insight::format_value(performance::r2(m)$R2, as_percent=TRUE)`. Based on this simple regression model, we can derive the following approximation (assuming a standardized signal with an SD of 1): 

`r equatiomatic::extract_eq(m, use_coefs = TRUE, ital_vars=TRUE, coef_digits=4)`

Interestingly, for a dimension *m* of 2, this equation approximates the *0.2 SD* heuristic (*r* = `r insight::format_value(predict(m, data.frame(Dimension=2)), digits=3)`), which actually was initially derived under this condition. 



### Heuristics Comparison

```{r warning=FALSE, message=FALSE}
df$NN_Target <- predict(m_NN, newdata=df)
df$RR_Target <- predict(m_RR, newdata=df)
   


# Stuff
data <- df |> 
  group_by(Dimension, Length, Iter) |> 
  summarise(maxApEn = mean(Optimal_maxApEn, na.rm=TRUE),
            SD = 0.2,
            Scholzel = mean(Optimal_Scholzel, na.rm=TRUE),
            Chon = mean(Optimal_Chon, na.rm=TRUE)) 

data$NN <- df |>  
  group_by(Dimension, Length, Iter) |> 
  filter(Method == "Nearest Neighbours") |> 
  filter(Score - NN_Target == min(Score - NN_Target)) |> 
  summarize(Tolerance = mean(Tolerance)) |> 
  ungroup() |> 
  pull(Tolerance)

data$RR <- df |>  
  group_by(Dimension, Length, Iter) |> 
  filter(Method == "Recurrence Rate") |> 
  filter(Score - RR_Target == min(Score - RR_Target)) |> 
  summarize(Tolerance = mean(Tolerance)) |> 
  ungroup() |> 
  pull(Tolerance)

data$Makowski <- predict(m, data=data)


SD <- lm(maxApEn ~ SD, data=data)
Scholzel <- lm(maxApEn ~ Scholzel, data=data)
Chon <- lm(maxApEn ~ Chon, data=data)
NN <- lm(maxApEn ~ NN, data=data)
RR <- lm(maxApEn ~ RR, data=data)
Makowski <- lm(maxApEn ~ Makowski, data=data)

perf <- compare_performance(SD, Scholzel, Chon, NN, RR, Makowski) 
perf$BF <- test_performance(SD, Scholzel, Chon, NN, RR, Makowski, reference = 6)$BF

perf |> 
  arrange(BIC) |> 
  select(Model = Name, 
         BIC,
         R2,
         BF) |> 
  insight::print_md()
```

We compared together different approximations of $r_{maxApEn}$ (see **Table 1**). Our heuristic method presented in this study, based on the signal's SD and the embedding dimension, surpassed any other models ($BF_{10} > 1000$, $R^2$ = `r format_value(r2(Makowski)$R2)`), followed by the *Schötzel* adjustment ($R^2$ = `r format_value(r2(Scholzel)$R2)`). The methods based on *RR* ($R^2$ = `r format_value(r2(RR)$R2)`) and *NN* ($R^2$ = `r format_value(r2(NN)$R2)`) were next, followed the *Chon* adjustment ($R^2$ = `r format_value(r2(Chon)$R2)`) and, finally, the fixed 0.2 SD value.


## Discussion

The tolerance threshold *r* is a key parameter of several entropy algorithms, including widely popular ones like *SampEn*. The current gold standard method to estimate the optimal *r* is to compute Approximate Entropy (*ApEn*) over a range of different *r* values and to select the one corresponding to the maximum *ApEn* value. Unfortunately, this method is computationally costly. 

In this study, we have shown that a simple heuristic approximation based on the signal's SD and the embedding dimension *m* was the best at approximating $r_{maxApEn}$, showing superior performance to procedures involving state-phase reconstruction related quantities, such as the amount of Nearest Neighbours (*NN*) and the Recurrence Rate (*RR*). We suggest the use of this method as a default alternative to the *0.2 SD* rule of thumb.

While we believe that our data generation procedure was able to generate a wide variety of signals, and that our results are to some extent generalizable, future studies could attempt at refining the estimation procedures for specific signals (for instance, EEG, or heart rate data). All the methods of optimal tolerance *r* estimation used in this study are available in the *NeuroKit2* open-source Python software [@makowski2021neurokit2]. 


\newpage

## References

::: {#refs custom-style="Bibliography"}
:::
