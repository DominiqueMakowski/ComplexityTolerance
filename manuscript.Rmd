---
title             : "**A New Heuristic Method for the Optimal Selection of Tolerance *r* for Entropy Indices**"
shorttitle        : "Optimal Tolerance"
author: 
  - name          : "Dominique Makowski"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    address       : "HSS 04-18, 48 Nanyang Avenue, Singapore"
    email         : "dmakowski@ntu.edu.sg"
    orcid         : 0000-0001-5375-9967
      
affiliation:
  - id            : "1"
    institution   : "School of Social Sciences, Nanyang Technological University, Singapore"
authornote: |
  Correspondence concerning this article should be addressed to Dominique Makowski, HSS 04-18, 48 Nanyang Avenue, Singapore (dom.makowski@gmail.com).
abstract: |
  The tolerance threshold *r* is a key parameter of several entropy algorithms (e.g., *SampEn*). Unfortunately, the gold standard method to estimate its optimal value - i.e., the one that maximizes *ApEn* - is computationally costly, prompting users to rely on cargo-cult rules-of-thumb such as 0.2 * SD. This simulation study aims at validating a new heuristic, based on the embedding dimension *m* and the signal's length *n* (optimal *r* = SD \* 0.281(*m*-1) + 0.005(log(*n*)) - 0.02(*m*-1 \* log(*n*))), which was found to be superior to other existing heuristics. All the methods of optimal tolerance *r* estimation used in this study are available in the *NeuroKit2* Python software (Makowski et al., 2021).
  
keywords          : "chaos, complexity, entropy, tolerance, r, physiology"
wordcount         : "925"
bibliography      : "bibliography.bib"
floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no
figurelist        : yes
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
csl: utils/mdpi.csl
header-includes:
  - \usepackage[labelfont=bf, font={scriptsize, color=gray}]{caption}
editor_options: 
  chunk_output_type: console
---



```{r, echo = FALSE, warning=FALSE, message=FALSE}
# options and parameters
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  dpi = 450,
  fig.width = see::golden_ratio(7),
  fig.height = 7,
  fig.path = "figures/"
)

cache <- TRUE
```


## Introduction

Complexity analysis is an increasingly popular approach to physiological signals, including cardiac (e.g., Heart Rate Variability, [@pham2021heart]) and brain activity [@lau2021brain]. It is an umbrella term for the usage of various complexity indices that quantify concepts such as chaos, entropy, fractal dimension, randomness, predictability, and information. Importantly, some of the most popular indices of entropy (e.g., *ApEn*, *SampEn*, their fuzzy and multiscale variations) and recurrence quantification analysis (RQA), rely on a similar set of parameters. Namely, these are the delay $\tau$, the embedding dimension *m*, and the tolerance *r*, which are critical to accurately capture the space in which complexity becomes quantifiable. Unfortunately, despite the existence of methods to estimate optimal values for these parameters depending on the signal at hand, their choice often relies on simple heuristics and cargo-cult conventions.

Such is the case of the tolerance threshold *r*, which typically corresponds to the minimal distance required to assign two points in a state-space as belonging to the same state. It directly impacts the amount of "recurrences" of a system and the measure of its tendency to revisit past states, which is the base metric for the calculation of the aforementioned entropy indices. Despite its importance, it is often selected as a function of the standard deviation (SD) of the signal, with (in)famous arbitrary values including $0.1$ or $0.2*SD$ [@pincus1992approximate]. One of the reason for the longevity of such an approach is 1) the past literature (as it is consistently used in the existing literature, the choice of the same values becomes the default and does not require much justification) and 2) the fact that other algorithms to estimate the optimal *r* are computationally costly.

<!-- Figure of 2D attractor with radius  -->

The aim of the present study is to investigate the relationship between different methods for optimal tolerance *r* estimation. The ground-truth method used is the tolerance value corresponding to a maximal value of Approximate Entropy - *ApEn* [@chen2008parameter; @lu2008automatic; @chon2009approximate]. As this method is computationally expensive, the objective of this study is to assess whether fast heuristic   proxies can be used to approximate $r_{maxApEn}$.


## Methods

```{r message=FALSE, warning=FALSE, cache=cache}
library(tidyverse)
library(easystats)
library(patchwork)


df <- data.frame()
for(i in 1:30) {
  df <- rbind(df, read.csv(paste0("data/data_Tolerance_part", i, ".csv")))
}

df <- df |> 
  mutate(Iter = paste0(Signal, Noise_Intensity, Noise_Type, Length)) |> 
  group_by(Method, Dimension, Iter) |> 
  mutate(Score_N = normalize(Score)) |> 
  ungroup()
```


For *n* = `r length(unique(df$Iter))` combinations of different signal types and lengths, as well as noise types and intensities (the procedure used was the same as in @makowski2022structure, and the data generation code is available at **https://github.com/DominiqueMakowski/ComplexityTolerance**), we computed the Approximate Entropy (*ApEn*), which peak is used to estimate the optimal tolerance level for time-delay embedding spaces ranging from of 1 to 9 embedding dimensions *m*.

<!-- we computed 3 different scores as a function of difference tolerance values (expressed in SDs of the signal): Approximate Entropy (*ApEn*), which peak is used to estimate the optimal tolerance level; the average number of Nearest Neighbours *NN*, which is the underlying quantity used by several entropy algorithms; and the Recurrence Rate *RR*, one of the core index of recurrence quantification analysis (RQA). These 3 scores were computed for time-delay embedding spaces ranging from of 1 to 9 embedding dimensions *m*. -->


The aim of the analysis is to establish a new heuristic based only on the signal's SD and the embedding dimension *m*; and compare all of these approximations with other existing heuristics such as *0.2 SD*, *Chon* [@chon2009approximate], and the *Schötzel* method ($1.3334 + 0.5627 * log(dimension)$) implemented in the package *nolds* [@scholzel2019nolds].

<!-- 1) investigate the possibility of using alternative scores, namely *RR* and *NN*, to approximate the location of the *ApEn* peak; 2) establish a new heuristic based only on the signal's SD and the embedding dimension *m*; and 3) compare all of these approximations with other existing heuristics such as *0.2 SD*, *Chon* [@chon2009approximate], and the *Schötzel* method ($1.3334 + 0.5627 * log(dimension)$) implemented in the package *nolds* [@scholzel2019nolds]. -->

<!-- NOLDS: (The rationale is that the chebyshev distance (used in various metrics) rises logarithmically with increasing dimension. ``0.5627 * np.log(dimension) + 1.3334`` is the logarithmic trend line for the chebyshev distance of vectors sampled from a univariate normal distribution. A constant of `0.1164`` is used so that ``tolerance = 0.2 * SDs`` for ``dimension = 2``) -->

## Results


### Maximum Approximate Entropy


```{r fig1, warning=FALSE, message=FALSE, cache=cache, eval=TRUE, fig.cap="Approximate Entropy *ApEn* as a function of tolerance *r* (expressed in signal SD) and embedding dimension *m*."}
p <- df |> 
  mutate(group = paste0(Dimension, Method, Iter),
         m = Dimension) |> 
  # filter(Method %in% c("Approximate Entropy", "Recurrence Rate", "Nearest Neighbours")) |> 
  filter(Method %in% c("Approximate Entropy")) |> 
  ggplot(aes(x = Tolerance, y = Score_N, color = Method)) +
  geom_line(aes(group=group), alpha=0.05, size=0.1) +
  facet_wrap(~m, labeller=purrr::partial(label_both, sep = " = ")) +
  scale_x_continuous(expand=c(0, 0)) +
  # scale_y_continuous(expand=c(0, 0), labels = scales::percent) +
  scale_y_continuous(expand=c(0, 0)) +
  scale_color_manual(values=c("Approximate Entropy" = "#9C27B0")) +
  # scale_color_manual(values=c("Approximate Entropy" = "#9C27B0", "Nearest Neighbours" = "#2196F3", "Recurrence Rate" = "#FF9800")) +
  see::theme_modern() + 
  theme(strip.text = element_text(face="italic"),
        # axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  # guides(colour = guide_legend(override.aes = list(alpha = 1, size=1))) +
  guides(colour = "none") +
  labs(x = expression("Tolerance"~italic("r")~"(in signal's SD)"), 
       y = "Approximate Entropy",
       color = "") 
p
# ggsave("figures/fig1-1.png", width=see::golden_ratio(7), height=7, dpi=300)
```
```{r fig1b, echo=FALSE, fig.cap="Approximate Entropy *ApEn* as a function of tolerance *r* (expressed in signal SD) and embedding dimension *m*.", message=FALSE, warning=FALSE, cache=FALSE, out.width="100%", eval=FALSE}
knitr::include_graphics("figures/fig1-1.png")
```

**Figure 1** shows the normalized value of Approximate Entropy *ApEn*
<!-- , the amount of nearest neighbours *NN* and the Recurrence Rate *RR*  -->
as a function of tolerance *r* and embedding dimension *m*. As expected, the value of *ApEn* peaks at certain values of *r* (hence its usage as an indicator of the optimal tolerance). The location of this peak seems strongly impacted by the embedding dimension *m*, getting more variable - and on average larger - as *m* increases. 
<!-- Does this peak consistently correspond to certain values of *NN* and *RR*? -->



### New Heuristic



```{r warning=FALSE, message=FALSE, include=FALSE}
data <- df |> 
  group_by(Dimension, Length, Iter) |> 
  summarise(maxApEn = mean(Optimal_maxApEn, na.rm=TRUE)) 


m1 <- lm(maxApEn ~ 0 + Dimension, data=mutate(data, Dimension = Dimension - 1))
m2 <- lm(maxApEn ~ 0 + log(Dimension), data=data)
m3 <- lm(maxApEn ~ 0 + Dimension * Length, data=mutate(data, Dimension = Dimension - 1))
m4 <- lm(maxApEn ~ 0 + log(Dimension) * Length, data=data)
m5 <- lm(maxApEn ~ 0 + Dimension * log(Length), data=mutate(data, Dimension = Dimension - 1))
m6 <- lm(maxApEn ~ 0 + log(Dimension) * log(Length), data=data)

rez <- test_bf(m1, m2, m3, m4, m5, m6, reference=5)
as.data.frame(rez)

m <- m5
```

<!-- Because computing *RR* or *NN* is also an expensive procedure, we also attempted at validating a new heuristic based only on the signal's *SD* and the embedding dimension *m*. -->



```{r fig2, warning=FALSE, message=FALSE, fig.cap="Optimal tolerance values approximated by a new heuristic model based on the embedding dimension *m* and the signal length *n* (in samples). The density plots show the true optimal tolernace values as based on max. ApEn."}
library(ggdist)

pred <- modelbased::estimate_relation(m, length = NA) |> 
  mutate(Length = as.factor(Length),
         Dimension = Dimension + 1)

formula <- equatiomatic::extract_eq(m, 
                                    raw_tex=TRUE, 
                                    swap_var_names=c("maxApEn" = "r", 
                                                     "Dimension" = "m-1", 
                                                     "log(Length)" = "log(n)"),
                                    use_coefs = TRUE, 
                                    ital_vars=TRUE, 
                                    coef_digits=4)
  
data |> 
  mutate(Dimension = as.factor(Dimension),
         Length = as.factor(Length)) |> 
  ggplot(aes(x=maxApEn, y=Dimension)) +
  ggdist::stat_halfeye(aes(fill=Length, color=Length, group=Length), adjust =2, point_interval="mode_qi", normalize="groups", alpha=0.5) +
  # geom_density_ridges(aes(fill=Dimension), color = NA) +
  # stat_density_ridges(aes(fill=Dimension), quantile_lines = TRUE, quantiles = 2, color = NA)
  geom_vline(xintercept=c(0.1, 0.2, 0.5, 1), linetype="dotted", size=0.5, alpha=0.5) +
  geom_line(data=pred, aes(x=Predicted, color=Length), size=1, show.legend=FALSE) +
  coord_flip() +
  annotate(geom="text", x=1.5, y=1.2, label=latex2exp::TeX(formula, italic=TRUE), hjust=0, color="black") +
  scale_fill_viridis_d(option="inferno") +
  scale_color_viridis_d(option="inferno") +
  scale_x_continuous(expand=c(0, 0), breaks = c(0.1, 0.2, 0.5, 1, 2)) +
  scale_y_discrete(expand=c(0, 0)) +
  see::theme_modern() +
  guides(group="none", fill=guide_legend(override.aes = list(alpha = 1))) +
  labs(x = expression("Optimal Tolerance"~italic("r")~"(based on max. ApEn)"), y=expression("Embedding Dimension"~italic("m")), fill="Signal Length", color="Signal Length")
```

Selecting the tolerance based on the signal's SD alone does not appear as a good default, given the strong impact of embedding dimension. In order to validate a new heuristic to approximate the optimal *r* value based on the embedding dimension *m* and the signal's length, we compared different regression specifications using the BIC-based Bayes Factor test. The model which included the log-transformed signal length (in samples) and the embedding dimension *m* minus 1 (with no intercept) performed significantly better ($BF_{10} > 1000$) than any other model, with an explained variance of `r insight::format_value(performance::r2(m)$R2, as_percent=TRUE)`. Based on this simple regression model, we can derive the following approximation (assuming a standardized signal with an SD of 1): 

`r formula`

It should be noted that shorter signals require larger tolerance values, and the impact of length lowers as the signal gets longer. Also, for an embedding dimension of 2 (and short signal lengths), this equation returns values close to the *0.2 SD* heuristic, which is not entirely surprising as the latter was initially derived under such conditions (and that are common in some applications, such as heart-rate intervals). 



### Heuristics Comparison

```{r warning=FALSE, message=FALSE}
# Stuff

# df$NN_Target <- predict(m_NN, newdata=df)
# df$RR_Target <- predict(m_RR, newdata=df)
#    
# data$NN <- df |>  
#   group_by(Dimension, Length, Iter) |> 
#   filter(Method == "Nearest Neighbours") |> 
#   filter(Score - NN_Target == min(Score - NN_Target)) |> 
#   summarize(Tolerance = mean(Tolerance)) |> 
#   ungroup() |> 
#   pull(Tolerance)
# 
# data$RR <- df |>  
#   group_by(Dimension, Length, Iter) |> 
#   filter(Method == "Recurrence Rate") |> 
#   filter(Score - RR_Target == min(Score - RR_Target)) |> 
#   summarize(Tolerance = mean(Tolerance)) |> 
#   ungroup() |> 
#   pull(Tolerance)

data <- df |> 
  group_by(Dimension, Length, Iter) |> 
  summarise(maxApEn = mean(Optimal_maxApEn, na.rm=TRUE),
            SD = 0.2,
            Scholzel = mean(Optimal_Scholzel, na.rm=TRUE),
            Chon = mean(Optimal_Chon, na.rm=TRUE)) 

data$Makowski <- predict(m, data=mutate(data, Dimension = Dimension - 1))


SD <- lm(maxApEn ~ SD, data=data)
Scholzel <- lm(maxApEn ~ Scholzel, data=data)
Chon <- lm(maxApEn ~ Chon, data=data)
# NN <- lm(maxApEn ~ NN, data=data)
# RR <- lm(maxApEn ~ RR, data=data)
Makowski <- lm(maxApEn ~ Makowski, data=data)

perf <- compare_performance(SD, Scholzel, Chon, Makowski) 
perf$BF <- test_performance(SD, Scholzel, Chon,Makowski, reference = 4)$BF

perf |> 
  arrange(BIC) |> 
  select(Model = Name, 
         BIC,
         AIC,
         R2) |> 
  insight::print_md()
```

We compared together different methods to approximate $r_{maxApEn}$ (see **Table 1**) by comparing $r_{maxApEn}$ (our ground-truth) to the values estimated by different methods. The new heuristic method introduced in this study, based on the signal's SD, the embedding dimension and the log-transformed length, surpassed all other models ($BF_{10} > 1000$, $R^2$ = `r format_value(r2(Makowski)$R2)`). 



## Discussion

The tolerance threshold *r* is a key parameter of several entropy algorithms, including widely popular ones like *SampEn*. The current gold standard method to estimate the optimal *r* is to compute Approximate Entropy (*ApEn*) over a range of different *r* values and to select the one corresponding to the maximum *ApEn* value. Unfortunately, this method is computationally costly. 

In this study, we showed that a simple heuristic approximation based on the embedding dimension *m* and the log-transformed signal length is a valid approximation of $r_{maxApEn}$, showing superior performance to other heuristic methods. 
<!-- procedures involving state-phase reconstruction related quantities, such as the amount of Nearest Neighbours (*NN*) and the Recurrence Rate (*RR*).  -->
We recommend the use of this method as a default alternative to the *0.2 SD* rule of thumb.

While we believe that our data generation procedure was able to generate a wide variety of signals, and that our results are to some extent generalizable, future studies could attempt at refining the estimation procedures for specific signals (for instance, EEG, or heart rate data). All the methods of optimal tolerance *r* estimation used in this study, including our new proposal, are available in the *NeuroKit2* open-source Python software, as an option in the `complexity_tolerance()` function [@makowski2021neurokit2]. 


\newpage

## References

::: {#refs custom-style="Bibliography"}
:::
